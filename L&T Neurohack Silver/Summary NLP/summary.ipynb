{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized Text:\n",
      "global warming is causing shifts in weather patterns, rising sea levels, and more frequent extreme weather events. the impacts of climate change are already being felt across the world, particularly in vulnerable communities that have contributed the least to the problem.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "# import sentencepiece\n",
    "# import torch\n",
    "\n",
    "\n",
    "# model_name = 't5-small'  \n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# input = (\"How would you like to Summarize your input : \")\n",
    "\n",
    "# def summarize_text(file_path, max_length=150, min_length=40):\n",
    "    \n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         text = file.read()\n",
    "\n",
    "    \n",
    "#     inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    \n",
    "#     summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "\n",
    "    \n",
    "#     summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "#     return summary\n",
    "\n",
    "\n",
    "# input_file = 'try.txt'\n",
    "\n",
    "\n",
    "# summarized_text = summarize_text(input_file)\n",
    "\n",
    "\n",
    "# print(\"Summarized Text:\")\n",
    "# print(summarized_text)\n",
    "\n",
    "\n",
    "# with open('summarized_output.txt', 'w', encoding='utf-8') as output_file:\n",
    "#     output_file.write(summarized_text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Flan-T5-Small \n",
    " | Model\t        | Size (Storage)\t| GPU Memory\t|  Inference Speed (GPU)\t|   Inference Speed (CPU)    |\n",
    " | Flan-T5-Small\t| ~242 MB\t        |  1-1.5 GB\t    |  < 1 second (512 tokens)\t|   2-5 seconds (512 tokens) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarized Text:\n",
      "Artificial intelligence has revolutionized the way businesses operate, transforming processes and enhancing efficiencies across multiple sectors. The advent of AI technologies has allowed companies to automate routine tasks, analyze large datasets, and derive actionable insights that drive decision-making. As businesses continue to adapt to rapidly changing market conditions, the integration of AI has become a crucial component for success. ### The Role of Artificial Intelligence in Modern Business\n",
      "\n",
      "Summarized text saved to 'summarized_output.txt'.\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "model_name = 'google/flan-t5-small'\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def analyze_content_structure(text):\n",
    "    \"\"\"\n",
    "    Analyze text to understand its structure, key elements, and suggest the best summarization format.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    num_entities = len(doc.ents)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    num_tokens = len(doc)\n",
    "    num_keywords = sum(1 for token in doc if not token.is_stop)\n",
    "    num_bullets = text.count('- ') + text.count('* ') + text.count('â€¢ ')\n",
    "    num_arrows = text.count('->') + text.count('=>')\n",
    "\n",
    "    \n",
    "    if num_bullets > 3 or 'list' in text.lower():\n",
    "        return \"bullet points\"\n",
    "    elif num_arrows > 2 or \"process\" in text.lower() or \"steps\" in text.lower():\n",
    "        return \"flowchart\"\n",
    "    elif num_entities > 5 and num_keywords / num_tokens < 0.5:\n",
    "        return \"table\"\n",
    "    else:\n",
    "        return \"paragraph\"\n",
    "\n",
    "def get_optional_context():\n",
    "    context = input(\"Would you like to provide additional context for better summarization? (optional, press enter to skip): \")\n",
    "    return context\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment\n",
    "    return sentiment.polarity, sentiment.subjectivity\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    text = html.unescape(text)    \n",
    "    text = text.replace('&nbsp;', ' ').replace('&lt;', '<').replace('&gt;', '>')\n",
    "    return text\n",
    "\n",
    "def clean_summary(summary, style):\n",
    "\n",
    "    if style == \"bullet points\":\n",
    "        cleaned_lines = []\n",
    "        for line in summary.split(\". \"):\n",
    "            line = line.strip().lstrip(\"1234567890.- \")\n",
    "            if line:\n",
    "                cleaned_lines.append(f\"- {line}\")\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "    elif style == \"flowchart\":\n",
    "        cleaned_lines = []\n",
    "        for line in summary.split(\". \"):\n",
    "            line = line.strip().lstrip(\"1234567890.- \")\n",
    "            if line:\n",
    "                cleaned_lines.append(f\"-> {line}\")\n",
    "        return \"\\n\".join(cleaned_lines)\n",
    "    else:\n",
    "        return summary.replace(\". \", \".\\n\")\n",
    "\n",
    "def summarize_section(section_text, context='', style='paragraph'):\n",
    "\n",
    "    section_text = clean_text(section_text)\n",
    "    combined_input = context + \"\\n\" + section_text if context else section_text    \n",
    "    prompt = f\"summarize the content as {style}: {combined_input}\"\n",
    "\n",
    "    input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    input_length = input_tokens.shape[1]\n",
    "    max_length = 1000 \n",
    "    min_length = 100 \n",
    "\n",
    "    summary_ids = model.generate(\n",
    "        input_tokens,\n",
    "        max_length=max_length,\n",
    "        min_length=min_length,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        no_repeat_ngram_size=3,  \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "def summarize_text(file_path, context=''):\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    text = clean_text(text)\n",
    "    sections = text.split(\"\\n## \")\n",
    "    summarized_sections = []\n",
    "\n",
    "    for section in sections:\n",
    "        \n",
    "        style = analyze_content_structure(section)\n",
    "        \n",
    "        section_summary = summarize_section(section, context, style)\n",
    "        \n",
    "        cleaned_summary = clean_summary(section_summary, style)\n",
    "        summarized_sections.append(f\"## {cleaned_summary}\")\n",
    "\n",
    "    \n",
    "    final_summary = \"\\n\\n\".join(summarized_sections)\n",
    "    \n",
    "    output = f\"\\n{'='*40}\\n\"\n",
    "    output += f\"Summarized Text:\\n{'-'*40}\\n\"\n",
    "    output += final_summary + \"\\n\"\n",
    "\n",
    "    return output\n",
    "\n",
    "def main():\n",
    "    \n",
    "    input_file = input(\"Enter the path to the text file you want to summarize: \")\n",
    "    # input_file = 'try.txt'\n",
    "    context = get_optional_context()\n",
    "    summarized_text = summarize_text(input_file, context)\n",
    "    print(summarized_text)\n",
    "\n",
    "    with open('summarized_output.txt', 'w', encoding='utf-8') as output_file:\n",
    "        output_file.write(summarized_text)\n",
    "    print(\"\\nSummarized text saved to 'summarized_output.txt'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ALL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
